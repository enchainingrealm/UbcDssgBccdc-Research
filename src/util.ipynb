{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_splits(df):\n",
    "    splits = []\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1731)\n",
    "    for train_indices, test_indices in kf.split(df):\n",
    "        df_train = df.iloc[train_indices, :]\n",
    "        df_test = df.iloc[test_indices, :]\n",
    "        \n",
    "        splits.append((df_train, df_test))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_X_y(vectorizer, df, label_name):\n",
    "    corpus = df[\"result_full_description\"]\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    y = df[label_name]\n",
    "    \n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_X_y_tt(vectorizer, df_train, df_test, label_name):\n",
    "    corpus_train = df_train[\"result_full_description\"]\n",
    "    corpus_test = df_test[\"result_full_description\"]\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(corpus_train)\n",
    "    X_test = vectorizer.transform(corpus_test)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    y_train = df_train[label_name]\n",
    "    y_test = df_test[label_name]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(classifier, X_train, X_test, y_train, y_test, df_test, classes):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred, classes).astype(int)\n",
    "    precision = get_precision(confusion)\n",
    "    recall = get_recall(confusion)\n",
    "    error = get_error(df_test, y_test, y_pred)\n",
    "    \n",
    "    return accuracy, confusion, precision, recall, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier_cv(classifier_factory, X_y_splits, classes):\n",
    "    c = len(classes)\n",
    "    \n",
    "    accuracy = 0\n",
    "    confusion = np.zeros((c, c)).astype(int)\n",
    "    errors = []\n",
    "    \n",
    "    for X_train, X_test, y_train, y_test, df_test in X_y_splits:\n",
    "        classifier = classifier_factory()\n",
    "        curr_accuracy, curr_confusion, _, _, curr_error = run_classifier(classifier, X_train, X_test, y_train, y_test, df_test, classes)\n",
    "        \n",
    "        accuracy += curr_accuracy\n",
    "        confusion += curr_confusion\n",
    "        errors.append(curr_error)\n",
    "    \n",
    "    k = len(X_y_splits)\n",
    "    accuracy /= k\n",
    "    \n",
    "    precision = get_precision(confusion)\n",
    "    recall = get_recall(confusion)\n",
    "    \n",
    "    error = pd.concat(errors)\n",
    "    return accuracy, confusion, precision, recall, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(confusion):\n",
    "    return np.diag(confusion) / np.sum(confusion, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(confusion):\n",
    "    return np.diag(confusion) / np.sum(confusion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(df_test, y_test, y_pred):\n",
    "    error_df = pd.concat([\n",
    "        df_test[\"result_full_description\"].reset_index(drop=True),\n",
    "        pd.DataFrame({\"y_true\": y_test}).reset_index(drop=True),\n",
    "        pd.DataFrame({\"y_pred\": y_pred}).reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    return error_df[\n",
    "        error_df[\"y_true\"] != error_df[\"y_pred\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_weights(classifier, feature_names):\n",
    "    feature_weights = [\n",
    "        (feature_names[index], weight)\n",
    "        for index, weight in enumerate(classifier.coef_[0])\n",
    "    ]\n",
    "    feature_weights.sort(key=lambda x: x[1])\n",
    "\n",
    "    min_weights = feature_weights[:10]\n",
    "    max_weights = feature_weights[-10:]\n",
    "    \n",
    "    return min_weights, max_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(classifier, feature_names):\n",
    "    feature_importances = [\n",
    "        (feature_names[index], importance)\n",
    "        for index, importance in enumerate(classifier.feature_importances_)\n",
    "    ]\n",
    "    feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "    return feature_importances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_candidates(candidates_str):\n",
    "    candidates_dict = json.loads(candidates_str)\n",
    "    candidates = set(candidates_dict.keys())\n",
    "    \n",
    "    banned = {\"Bacteria\", \"Virus\"}\n",
    "    candidates.difference_update(banned)\n",
    "    \n",
    "    return {candidate.lower() for candidate in candidates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one(_set):\n",
    "    assert _set   # _set is not empty\n",
    "    return next(iter(_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
